{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c763e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "cfg = json.load(open('api_config.json'))\n",
    "openai.api_key = cfg['KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a618d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "EMBED_MAX_TOKEN_LENGTH = 8000\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Cuts texts down to maximum token length\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > EMBED_MAX_TOKEN_LENGTH:\n",
    "        text = encoding.decode(tokens[:EMBED_MAX_TOKEN_LENGTH])\n",
    "    \n",
    "    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "# Possibly give random sampling of titles to get context before classifying?\n",
    "def get_cluster_label(prompt, model=\"gpt-3.5-turbo\", role=\"system\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": role, \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "LLM_MAX_TOKEN_LENGTH = 4000\n",
    "SUMAMRY_PROMPT = \"write a few bullet point synopsis about this article for technical readers: \"\n",
    "def get_bullet_summary(text, model=\"gpt-3.5-turbo\", role=\"system\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > LLM_MAX_TOKEN_LENGTH:\n",
    "        text = encoding.decode(tokens[:LLM_MAX_TOKEN_LENGTH])\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": role, \"content\": SUMAMRY_PROMPT + text},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b9c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/text_data.csv', sep='\\t')\n",
    "\n",
    "# Drops all empty text rows\n",
    "df = df[df['title'] != ' '].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96363137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc()[0]['title_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38b1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('embed_data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a2ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate title embeddings\n",
    "df['title_embedding'] = df['title'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "df.to_csv('embed_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a73eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text embeddings\n",
    "df['text_embedding'] = df['text'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "df.to_csv('embed_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c855985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bullet summaries\n",
    "df['summary'] = df['text'].apply(lambda x: get_bullet_summary(x))\n",
    "df.to_csv('embed_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f13cbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-Means Clustering\n",
    "\"\"\"\n",
    "matrix = np.vstack(df['title_embedding'].to_list())\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = int(len(df['title']) / 5)\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\n",
    "kmeans.fit(matrix)\n",
    "labels = kmeans.labels_\n",
    "df[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa23d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Labels each cluster with a overall topic.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = f'''I am going to give you a list of titles of similar articles about artificial intelligence.\n",
    "All of these will be discussing different aspects of AI, so get as specific as possible.\n",
    "I would like you to provide a short description of the group of articles and a few word label for the group.\n",
    "This label should refer to the domain where AI is being applied in the articles. Some examples are:\n",
    "\"AI in Law\", \"Facial Recognition\", \"Quantum Computing\", or \"AI in Culture and Art. \n",
    "\n",
    "Format your response like this:\n",
    "\n",
    "description: <short description>\n",
    "label: <one word label>\n",
    "\n",
    "here is the list of article titles: \n",
    "'''\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for cluster_ind in df['cluster'].unique():\n",
    "    # Gets the subframe of a specific cluster.\n",
    "    c_df = df[df['cluster'] == cluster_ind]\n",
    "    \n",
    "    titles = []\n",
    "    for t in list(c_df['title']):\n",
    "        titles.append(t)\n",
    "        \n",
    "    \n",
    "    prompts.append(PROMPT_TEMPLATE + str(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56a704f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-gpt-3.5-turbo in organization org-hn0Bt6qeMvPrNm670kbD7PtM on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bailey\\Projects\\ai_space\\embeddings\\create_embeddings.ipynb Cell 11\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m responses \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     responses\u001b[39m.\u001b[39mappend(get_cluster_label(prompt))\n",
      "\u001b[1;32mc:\\Users\\bailey\\Projects\\ai_space\\embeddings\\create_embeddings.ipynb Cell 11\u001b[0m line \u001b[0;36mget_cluster_label\u001b[1;34m(prompt, model, role)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cluster_label\u001b[39m(prompt, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, role\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: role, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bailey/Projects/ai_space/embeddings/create_embeddings.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-hn0Bt6qeMvPrNm670kbD7PtM on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Query GPT for cluster labels based on titles.\n",
    "\"\"\"\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    responses.append(get_cluster_label(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5de7b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a dictionary for cluster information.\n",
    "\"\"\"\n",
    "\n",
    "# ChatGPT generated function.\n",
    "def get_substring_between_strings(input_string, start_string, end_string):\n",
    "    start_index = input_string.find(start_string)\n",
    "    if start_index == -1:\n",
    "        return None  # Start string not found in the input string\n",
    "\n",
    "    end_index = input_string.find(end_string, start_index + len(start_string))\n",
    "    if end_index == -1:\n",
    "        return None  # End string not found in the input string\n",
    "\n",
    "    substring = input_string[start_index + len(start_string):end_index]\n",
    "    return substring\n",
    "\n",
    "cluster_labels = dict()\n",
    "\n",
    "for r, cluster_ind in zip(responses, df['cluster'].unique()):\n",
    "    response = r.choices[0].message.content\n",
    "    \n",
    "    description = get_substring_between_strings(response, 'description: ', 'label: ').replace(\"\\n\", \"\")\n",
    "    label = response.split('label: ')[1].replace(\"\\n\", \"\")\n",
    "    \n",
    "    cluster_labels[int(cluster_ind)] = {'label': label, 'desc': description}\n",
    "    \n",
    "with open('cluster_labels.json', 'w') as f:\n",
    "    json.dump(cluster_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13ab9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adds cluster data to the dataframe.\n",
    "\"\"\"\n",
    "\n",
    "cluster_labels = dict()\n",
    "with open ('cluster_labels.json', 'r') as f:\n",
    "    cluster_labels = json.load(f)\n",
    "\n",
    "df['cluster_info'] = df['cluster'].astype(str).map(cluster_labels)\n",
    "df['cluster_label'] = df['cluster_info'].apply(lambda x: x['label'])\n",
    "df['cluster_desc'] = df['cluster_info'].apply(lambda x: x['desc'])\n",
    "\n",
    "df.to_csv('cluster_data.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa44083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
